---
title: "Introduction to gadget & Benchmarks"
author: "Zizheng Zhang"
date: "2025-06-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(gadget)
library(iml)
library(mlr3)
library(mlr3learners)
library(ranger)
library(bench)
library(future)
library(ggplot2)
library(pryr)
```

## Overview of the gadget Package

The **gadget** package provides a framework for building interpretable, regionally-partitioned decision trees based on local feature effect estimates (such as ICE or PDP curves). The core workflow is as follows:

-   **Feature Effect Calculation**: Use external tools (e.g., the `iml` package) to compute local feature effects for a fitted machine learning model.
-   **Tree Construction**: Instantiate a `gadgetTree` object and use its `$fit()` method to recursively partition the data space, optimizing for regional homogeneity in feature effects. Each node is represented by a `Node` object.
-   **Visualization**: Use the tree's `$plot()` and `$plot_tree_structure()` methods to visualize the partial dependence or ICE behavior of features in each region of the tree, and the tree topology and splits.
-   **Split Information Extraction**: Use the tree's `$extract_split_info()` method to summarize the split criteria, node statistics, and regional effect heterogeneity for interpretation and reporting.

The package is modular and extensible: different effect strategies (e.g., partial dependence, accumulated local effects) can be implemented by extending the strategy interface. This design allows users to interpret complex black-box models by partitioning the feature space into regions with distinct, interpretable effect patterns.

## Synthetic data

### Get feature effects 

```{r}
set.seed(123)
n = 5000
x1 = runif(n, -1, 1)
x2 = runif(n, -1, 1)
x3 = runif(n, -1, 1)
y = ifelse(x3 > 0, 3 * x1, -3 * x1) + x3 + rnorm(n, sd = 0.3)
syn.data = data.frame(x1, x2, x3, y)

syn.task = TaskRegr$new("xor", backend = syn.data, target = "y")
syn.learner = lrn("regr.ranger")
syn.learner$train(syn.task)
syn.predictor = Predictor$new(syn.learner, data = syn.data[, c("x1", "x2", "x3")], y = syn.data$y)
syn.effect = FeatureEffects$new(syn.predictor, grid.size = 20, method = "ice")
```

### Fit and visualize the explanation tree

```{r}
tree = gadgetTree$new(strategy = pdStrategy$new(), n.split = 4, impr.par = 0.1, min.node.size = 1)
tree$fit(syn.effect, syn.data, target.feature.name = "y")
tree$plot(syn.effect, syn.data, target.feature.name = "y",
  show.plot = TRUE, show.point = FALSE, mean.center = TRUE)
tree$plot_tree_structure()
tree$extract_split_info()
```

## Bikeshare data

### Get feature effects 

```{r}
library(ISLR2)
data(Bikeshare)
set.seed(123)
bike = data.table(Bikeshare[sample(1:8645, 1000), ])

bike.X = bike[, .(day, hr, temp, windspeed, weekday, workingday, hum, season, mnth, holiday, registered, weathersit, atemp, casual)]
bike.y = bike$bikers
train = cbind(bike.X, "target" = bike.y)
bike.data = as.data.frame(train)

set.seed(123)
bike.task = TaskRegr$new(id = "bike", backend = bike.data, target = "target")
bike.learner = lrn("regr.ranger")
bike.learner$train(bike.task)

bike.X = bike.task$data(cols = bike.task$feature_names)
bike.y = bike.task$data(cols = bike.task$target_names)[[1]]

bike.predictor = Predictor$new(model = bike.learner, data = bike.X, y = bike.y)

effect_all = FeatureEffects$new(bike.predictor, method = "ice",
  grid.size = 20)
```

### Fit and visualize the explanation tree

```{r}
tree = gadgetTree$new(strategy = pdStrategy$new(), n.split = 4)
tree$fit(effect_all, bike.data, target.feature.name = "target")
tree$plot_tree_structure()
tree$extract_split_info()
tree$plot(effect_all, bike.data, target.feature.name = "target",
  show.plot = TRUE, show.point = TRUE, mean.center = FALSE,
  depth = 4,
  node.id = c(15, 14),
  features = c("hr", "workingday")
)
```

### Speed
```{r}
esi = tree$extract_split_info()
boxplot(time ~ depth, data = esi, main = "Distribution of split time per depth")
```

## gadgetTree fit benchmark

### Data generation

```{r}
set.seed(1)
options(future.globals.maxSize = 4 * 1024 * 1024^2) # 4GB
plan(sequential)

datagen_p5 = function(n, seed = 1) {
  set.seed(seed)
  x1 = round(runif(n, -1, 1), 1)
  x2 = round(runif(n, -1, 1), 3)
  x3 = as.factor(sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5)))
  x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))
  x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  dat = data.frame(x1, x2, x3, x4, x5)
  y = 0.2 * x1 - 8 * x2 + ifelse(x3 == 0, 16 * x2, 0) + ifelse(x1 > 0, 8 * x2, 0)
  eps = rnorm(n, 0, 0.1 * sd(y))
  y = y + eps
  dat$y = y
  X = dat[, setdiff(colnames(dat), "y")]
  mod = ranger(y ~ ., data = dat, num.trees = 100)
  pred = function(model, newdata) predict(model, newdata)$predictions
  model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
  eff = FeatureEffects$new(model, method = "ice", grid.size = 20)
  list(dat = dat, eff = eff)
}

datagen_p10 = function(n, seed = 1) {
  set.seed(seed)
  x1 = round(runif(n, -1, 1), 1)
  x2 = round(runif(n, -1, 1), 3)
  x3 = as.factor(sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5)))
  x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))
  x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  x6 = rnorm(n, mean = 1, sd = 5)
  x7 = round(rnorm(n, mean = 10, sd = 10), 2)
  x8 = round(rnorm(n, mean = 100, sd = 15), 4)
  x9 = round(rnorm(n, mean = 1000, sd = 20), 7)
  x10 = rnorm(n, mean = 10000, sd = 25)
  dat = data.frame(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
  y = 0.2 * x1 - 8 * x2 + ifelse(x3 == 0, 16 * x2, 0) + ifelse(x1 > 0, 8 * x2, 0) + 2 * x8
  eps = rnorm(n, 0, 0.1 * sd(y))
  y = y + eps
  dat$y = y
  X = dat[, setdiff(colnames(dat), "y")]
  mod = ranger(y ~ ., data = dat, num.trees = 100)
  pred = function(model, newdata) predict(model, newdata)$predictions
  model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
  eff = FeatureEffects$new(model, method = "ice", grid.size = 20)
  list(dat = dat, eff = eff)
}

datagen_p20 = function(n, seed = 1) {
  set.seed(seed)
  x1 = round(runif(n, -1, 1), 1)
  x2 = round(runif(n, -1, 1), 3)
  x3 = as.factor(sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5)))
  x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))
  x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  x6 = rnorm(n, mean = 1, sd = 5)
  x7 = round(rnorm(n, mean = 10, sd = 10), 2)
  x8 = round(rnorm(n, mean = 100, sd = 15), 4)
  x9 = round(rnorm(n, mean = 1000, sd = 20), 7)
  x10 = rnorm(n, mean = 10000, sd = 25)
  noise = replicate(10, rnorm(n), simplify = FALSE)
  names(noise) = paste0("noise", 1:10)
  dat = data.frame(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, noise)
  y = 0.2 * x1 - 8 * x2 + ifelse(x3 == 0, 16 * x2, 0) + ifelse(x1 > 0, 8 * x2, 0) + 2 * x8
  eps = rnorm(n, 0, 0.1 * sd(y))
  y = y + eps
  dat$y = y
  X = dat[, setdiff(colnames(dat), "y")]
  mod = ranger(y ~ ., data = dat, num.trees = 100)
  pred = function(model, newdata) predict(model, newdata)$predictions
  model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
  eff = FeatureEffects$new(model, method = "ice", grid.size = 20)
  list(dat = dat, eff = eff)
}
```

### Run experiments

```{r}
n_list = c(1000, 5000, 10000)
p_list = c(5, 10, 20)

bench_results = list()
tree_sizes = data.frame(n = integer(), p = integer(), tree_size_MB = numeric())
for (n in n_list) {
  for (p in p_list) {
    cat(sprintf("Running: n = %d, p = %d\n", n, p))
    if (p == 5) {
      sim = datagen_p5(n)
    } else if (p == 10) {
      sim = datagen_p10(n)
    } else if (p == 20) {
      sim = datagen_p20(n)
    }
    tree = gadgetTree$new(strategy = pdStrategy$new(), n.split = 10)
    tree$fit(sim$eff, sim$dat, target.feature.name = "y")
    tree_size_MB = as.numeric(pryr::object_size(tree)) / 1024^2
    tree_sizes = rbind(tree_sizes, data.frame(n = n, p = p, tree_size_MB = tree_size_MB))
    # benchmark
    res = bench::mark(
      fit = {
        tree = gadgetTree$new(strategy = pdStrategy$new(), n.split = 10)
        tree$fit(sim$eff, sim$dat, target.feature.name = "y")
      },
      iterations = 5
    )
    res$n = n
    res$p = p
    bench_results[[paste0("n", n, "_p", p)]] = res
  }
}
```

### Collect and visualize results

```{r}
bench_all = do.call(rbind, bench_results)
n_vec = rep(bench_all$n, each = 5)
p_vec = rep(bench_all$p, each = 5)
time_vec = unlist(bench_all$time)
time_ms = as.numeric(time_vec) * 1000
bench_long = data.frame(
  n = n_vec,
  p = p_vec,
  time_ms = time_ms
)

ggplot(bench_long, aes(x = factor(n), y = time_ms, color = factor(p), group = p)) +
  geom_boxplot(aes(group = interaction(n, p))) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(x = "Sample Size (n)", y = "Fit Time (ms)", color = "Feature Number (p)",
       title = "gadgetTree$fit(n.split = 10) Benchmark: Varying n and p") +
  theme_minimal()
```


```{r}
tree_sizes
```

## Simplified Risk Calculation

$$
\begin{aligned}
SSE &= \sum_{i=1}^n(y_i-\bar{y})^2\\
&= \sum_{i=1}^ny_i^2-2\bar{y}\sum_{i=1}^ny_i+\sum^n\bar{y}^2\\
&= \sum_{i=1}^ny_i^2-2\bar{y}n\bar{y}+n\bar{y}^2\\
&= \sum_{i=1}^ny_i^2 - n\bar{y}^2\\
&= \sum_{i=1}^ny_i^2-n(\frac{\sum_{i=1}^ny_i}{n})^2\\
&= \sum_{i=1}^ny_i^2-\frac{1}{n}(\sum_{i=1}^ny_i)^2\\
&= SS -\frac{S^2}{n}
\end{aligned}
$$
$$
\begin{aligned}
SSE_{Reduction} &= SSE_{parent}-SSE_{left}-SSE_{right}\\
&= SS_{parent} -\frac{S_{parent}^2}{n_{parent}}-(SS_{left} -\frac{S_{left}^2}{n_{left}})-(SS_{right} -\frac{S_{right}^2}{n_{right}})
\end{aligned}
$$
Since $$n\_{parent} = n\_{left} + n\_{right}\\ SS\_{parent} = SS\_{left} + SS\_{right}$$

Then $$ 
SSE\_{Reduction} = -\frac{S_{parent}^2}{n_{parent}} +\frac{S_{left}^2}{n_{left}} +\frac{S_{right}^2}{n_{right}}\\ max(SSE\_{Reduction}) = max(\frac{S_{left}^2}{n_{left}} +\frac{S_{right}^2}{n_{right}})=min(-\frac{S_{left}^2}{n_{left}} -\frac{S_{right}^2}{n_{right}}) 
$$
