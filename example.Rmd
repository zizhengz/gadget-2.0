---
title: "Introduction to gadget - PDP/ALE Method & Benchmarks"
author: "Zizheng Zhang"
date: "2025-12-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(gadget)
library(iml)
library(mlr3)
library(mlr3learners)
library(ranger)
library(ISLR2)
library(bench)
library(future)
library(ggplot2)
library(pryr)
```

## Overview of the gadget Package

**GADGET** algorithm is designed to interpret black-box models by decomposing the feature space into regions where **interaction effects** are minimized, such that the obtained prediction subregions are dominated by **main effects**. The **gadget** package provides users an extensible framework to have an interpretable and regionally-partitioned decision tree based on different local feature effect estimation methods, current we have implementation based on the following methods:

-   Individual Conditional Expectation (**ICE**)

-   Partial Dependence (**PD**)

-   Accumulated Local Effect (**ALE**)

Both ALE and PD show the average model prediction over the feature. The difference is that ALE are computed as accumulated differences over the conditional prediction distribution and PD over the marginal distribution. ICE curves describe how would one single observation change when one feature changes and can be combined with PD plots (for all observations).

These effects can be pre-computed by the **iml** package or within the gadget package itself, and are then passed as input to the gadget `$fit()` function.

The gadget workflow would be as follows:

1.  **Feature Effect Selection**: Choose a suitable feature effect method

2.  **Feature Effect Computation**: Compute local feature effects for all (or subset of) observations from a fitted machine learning model either externally (e.g. for ICE/ PD) or internally (e.g. for ALE).

3.  **(Optional) Feature Set Selection**: Define interacting feature set (feature of interest) and splitting feature set. Interacting features and splitting features can be distinct, or partially, or fully overlap with each other. By default we use all features as both interacting feature set and splitting feature set.

4.  **Tree Construction**: Initialize a `gadgetTree` object and use its `$fit()` method to recursively partition the feature space, minimizing regional feature interactions within predefined features of interest.

5.  **Visualization**: Use `$plot()` to visualize the feature effects in each node, and `$plot_tree_structure()` to display the tree's topology and split points.

6.  **Split Information Extraction**: Use `$extract_split_info()` method to summarize the tree structure and node statistics for interpretation and reporting.

## Synthetic data

### ICE/PDP Method: Get feature effects

```{r}
set.seed(1234)
n = 500
x1 = runif(n, -1, 1)
x2 = runif(n, -1, 1)
x3 = runif(n, -1, 1)
y = ifelse(x3 > 0, 3 * x1, -3 * x1) + x3 + rnorm(n, sd = 0.3)
syn.data = data.frame(x1, x2, x3, y)

syn.task = TaskRegr$new("xor", backend = syn.data, target = "y")
syn.learner = lrn("regr.ranger")
syn.learner$train(syn.task)
syn.predictor = Predictor$new(syn.learner, data = syn.data[, c("x1", "x2", "x3")], y = syn.data$y)
syn.effect = FeatureEffects$new(syn.predictor, grid.size = 20, method = "ice")
```

### ICE/PDP Method: Fit and visualize the explanation tree

```{r}
syn.tree.pd = gadgetTree$new(strategy = pdStrategy$new(), n.split = 4, impr.par = 0.1, min.node.size = 1)
syn.tree.pd$fit(effect = syn.effect, data = syn.data, target.feature.name = "y")
syn.tree.pd$plot_tree_structure()
syn.esi.pd = syn.tree.pd$extract_split_info()
print(syn.esi.pd)
syn.tree.pd$plot(syn.effect, syn.data, target.feature.name = "y",
  show.plot = TRUE, show.point = T, mean.center = TRUE)
```

### ALE Method

```{r}
syn.tree.ale = gadgetTree$new(strategy = aleStrategy$new(), n.split = 3)
syn.tree.ale$fit(model = syn.learner, data = syn.data, target.feature.name = "y", n.intervals = 10)
syn.tree.ale$plot_tree_structure()
syn.esi.ale = syn.tree.ale$extract_split_info()
print(syn.esi.ale)
object_size(syn.tree.ale)
```

```{r}
syn.tree.ale$plot(
  data   = syn.data,
  target.feature.name = "y",
  depth   = NULL,
  show.plot = TRUE
)
```


## Bikeshare data

### ICE/PDP Method: Get feature effects

```{r}
library(ISLR2)
data(Bikeshare)
set.seed(123)
bike = data.table(Bikeshare[sample(1:8645, 1000), ])
bike[, workingday := as.factor((workingday))]
# working and hr is factor

# bike.X = bike[, .(day, hr, temp, windspeed, weekday, workingday, hum, season, mnth, holiday, registered, weathersit, atemp, casual)]
bike.X = bike[, .(hr, temp, workingday)]
bike.y = bike$bikers
train = cbind(bike.X, "target" = bike.y)
bike.data = as.data.frame(train)

set.seed(123)
bike.task = TaskRegr$new(id = "bike", backend = bike.data, target = "target")
bike.learner = lrn("regr.ranger")
bike.learner$train(bike.task)

bike.X = bike.task$data(cols = bike.task$feature_names)
bike.y = bike.task$data(cols = bike.task$target_names)[[1]]

bike.predictor = Predictor$new(model = bike.learner, data = bike.X, y = bike.y)

effect.all = FeatureEffects$new(bike.predictor, method = "ice",
  grid.size = 20)
```

### ICE/PDP Method: Fit and visualize the explanation tree

```{r}
bike.tree.pd = gadgetTree$new(strategy = pdStrategy$new(), n.split = 4)
bike.tree.pd$fit(effect = effect.all, data = bike.data, target.feature.name = "target")
bike.tree.pd$plot_tree_structure()
bike.esi.pd = bike.tree.pd$extract_split_info()
print(bike.esi.pd)
bike.tree.pd$plot(effect.all, bike.data, target.feature.name = "target",
  show.plot = TRUE, show.point = TRUE, mean.center = FALSE,
  depth = c(2,3),
  node.id = 2:7,
  features = c("hr", "temp")
)
```

### ALE Method

```{r}
bike.tree.ale = gadgetTree$new(strategy = aleStrategy$new(), impr.par = 0.01, n.split = 2)
bike.tree.ale$fit(model = bike.learner, data = bike.data, target.feature.name = "target", n.intervals = 10)
bike.tree.ale$plot_tree_structure()
bike.esi.ale = bike.tree.ale$extract_split_info()
print(bike.esi.ale)
```

```{r}
bike.tree.ale$plot(
  data   = bike.data,
  target.feature.name = "target",
  #depth   = NULL,
  #node.id = 2,
  mean.center = FALSE,
  show.point = TRUE,
  show.plot = TRUE
)
```

```{r}
object_size(bike.esi.ale)
```

## Speed

```{r}
boxplot(time ~ depth, data = syn.esi.pd, main = "Distribution of split time per depth - Syn.PD")
boxplot(time ~ depth, data = syn.esi.ale, main = "Distribution of split time per depth - Syn.ALE")
boxplot(time ~ depth, data = bike.esi.pd, main = "Distribution of split time per depth - Bike.PD")
boxplot(time ~ depth, data = bike.esi.ale, main = "Distribution of split time per depth - Bike.ALE")
```

## gadgetTree fit benchmark

### Data generation

```{r}
set.seed(1)
options(future.globals.maxSize = 4 * 1024 * 1024^2) # 4GB
plan(sequential)

datagen_p5 = function(n, seed = 1) {
  set.seed(seed)
  x1 = round(runif(n, -1, 1), 1)
  x2 = round(runif(n, -1, 1), 3)
  x3 = as.factor(sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5)))
  x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))
  x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  dat = data.frame(x1, x2, x3, x4, x5)
  y = 0.2 * x1 - 8 * x2 + ifelse(x3 == 0, 16 * x2, 0) + ifelse(x1 > 0, 8 * x2, 0)
  eps = rnorm(n, 0, 0.1 * sd(y))
  y = y + eps
  dat$y = y
  X = dat[, setdiff(colnames(dat), "y")]
  mod = ranger(y ~ ., data = dat, num.trees = 100)
  pred = function(model, newdata) predict(model, newdata)$predictions
  model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
  eff = FeatureEffects$new(model, method = "ice", grid.size = 20)
  list(dat = dat, eff = eff)
}

datagen_p10 = function(n, seed = 1) {
  set.seed(seed)
  x1 = round(runif(n, -1, 1), 1)
  x2 = round(runif(n, -1, 1), 3)
  x3 = as.factor(sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5)))
  x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))
  x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  x6 = rnorm(n, mean = 1, sd = 5)
  x7 = round(rnorm(n, mean = 10, sd = 10), 2)
  x8 = round(rnorm(n, mean = 100, sd = 15), 4)
  x9 = round(rnorm(n, mean = 1000, sd = 20), 7)
  x10 = rnorm(n, mean = 10000, sd = 25)
  dat = data.frame(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
  y = 0.2 * x1 - 8 * x2 + ifelse(x3 == 0, 16 * x2, 0) + ifelse(x1 > 0, 8 * x2, 0) + 2 * x8
  eps = rnorm(n, 0, 0.1 * sd(y))
  y = y + eps
  dat$y = y
  X = dat[, setdiff(colnames(dat), "y")]
  mod = ranger(y ~ ., data = dat, num.trees = 100)
  pred = function(model, newdata) predict(model, newdata)$predictions
  model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
  eff = FeatureEffects$new(model, method = "ice", grid.size = 20)
  list(dat = dat, eff = eff)
}

datagen_p20 = function(n, seed = 1) {
  set.seed(seed)
  x1 = round(runif(n, -1, 1), 1)
  x2 = round(runif(n, -1, 1), 3)
  x3 = as.factor(sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5)))
  x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))
  x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
  x6 = rnorm(n, mean = 1, sd = 5)
  x7 = round(rnorm(n, mean = 10, sd = 10), 2)
  x8 = round(rnorm(n, mean = 100, sd = 15), 4)
  x9 = round(rnorm(n, mean = 1000, sd = 20), 7)
  x10 = rnorm(n, mean = 10000, sd = 25)
  noise = replicate(10, rnorm(n), simplify = FALSE)
  names(noise) = paste0("noise", 1:10)
  dat = data.frame(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, noise)
  y = 0.2 * x1 - 8 * x2 + ifelse(x3 == 0, 16 * x2, 0) + ifelse(x1 > 0, 8 * x2, 0) + 2 * x8
  eps = rnorm(n, 0, 0.1 * sd(y))
  y = y + eps
  dat$y = y
  X = dat[, setdiff(colnames(dat), "y")]
  mod = ranger(y ~ ., data = dat, num.trees = 100)
  pred = function(model, newdata) predict(model, newdata)$predictions
  model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
  eff = FeatureEffects$new(model, method = "ice", grid.size = 20)
  list(dat = dat, eff = eff)
}
```

### Run experiments

```{r}
n_list = c(1000, 5000, 10000)
p_list = c(5, 10, 20)

bench_results = list()
tree_sizes = data.frame(n = integer(), p = integer(), tree_size_MB = numeric(), 
                       mem_before_MB = numeric(), mem_after_MB = numeric(), mem_increase_MB = numeric())
# Initial memory cleanup
gc()
initial_mem = gc()["Vcells", "used"]
for (n in n_list) {
  for (p in p_list) {
    cat(sprintf("Running: n = %d, p = %d\n", n, p))
    # Clean memory and record starting state
    gc()
    mem_before = gc()["Vcells", "used"]
    # Data generation
    if (p == 5) {
      sim = datagen_p5(n)
    } else if (p == 10) {
      sim = datagen_p10(n)
    } else if (p == 20) {
      sim = datagen_p20(n)
    }
    # Clean memory after data generation
    gc()
    # Create and fit tree
    tree = gadgetTree$new(strategy = pdStrategy$new(), n.split = 10)
    tree$fit(effect = sim$eff, data = sim$dat, target.feature.name = "y")
    # Clean memory after tree fitting
    gc()
    # Calculate tree size and memory usage
    tree_size_MB = as.numeric(pryr::object_size(tree)) / 1024^2
    mem_after = gc()["Vcells", "used"]
    mem_increase = mem_after - mem_before
    # Record results
    tree_sizes = rbind(tree_sizes, data.frame(
      n = n, 
      p = p, 
      tree_size_MB = tree_size_MB,
      mem_before_MB = mem_before,
      mem_after_MB = mem_after,
      mem_increase_MB = mem_increase
    ))
    # Clean up tree object
    rm(tree)
    gc()
    # Benchmark with memory monitoring
    res = bench::mark(
      fit = {
        # Clean memory
        gc()
        tree = gadgetTree$new(strategy = pdStrategy$new(), n.split = 10)
        tree$fit(effect = sim$eff, data = sim$dat, target.feature.name = "y")
        # Clean memory
        gc()
      },
      iterations = 5
    )
    res$n = n
    res$p = p
    bench_results[[paste0("n", n, "_p", p)]] = res
    # Clean up sim data
    rm(sim)
    gc()
    cat(sprintf("Memory used: %.2f MB, Tree size: %.2f MB\n", mem_increase, tree_size_MB))
  }
}

# Final memory cleanup
gc()
final_mem = gc()["Vcells", "used"]
cat(sprintf("Total memory increase: %.2f MB\n", final_mem - initial_mem))
```

### Collect and visualize results

```{r}
bench_all = do.call(rbind, bench_results)
n_vec = rep(bench_all$n, each = 5)
p_vec = rep(bench_all$p, each = 5)
time_vec = unlist(bench_all$time)
time_ms = as.numeric(time_vec) * 1000
bench_long = data.frame(
  n = n_vec,
  p = p_vec,
  time_ms = time_ms
)

ggplot(bench_long, aes(x = factor(n), y = time_ms, color = factor(p), group = p)) +
  geom_boxplot(aes(group = interaction(n, p))) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(x = "Sample Size (n)", y = "Fit Time (ms)", color = "Feature Number (p)",
       title = "gadgetTree$fit(n.split = 10) Benchmark: Varying n and p") +
  theme_minimal()
```

```{r}
tree_sizes
```

## Simplified Risk Calculation

$$
\begin{aligned}
SSE &= \sum_{i=1}^n(y_i-\bar{y})^2\\
&= \sum_{i=1}^ny_i^2-2\bar{y}\sum_{i=1}^ny_i+\sum^n\bar{y}^2\\
&= \sum_{i=1}^ny_i^2-2\bar{y}n\bar{y}+n\bar{y}^2\\
&= \sum_{i=1}^ny_i^2 - n\bar{y}^2\\
&= \sum_{i=1}^ny_i^2-n(\frac{\sum_{i=1}^ny_i}{n})^2\\
&= \sum_{i=1}^ny_i^2-\frac{1}{n}(\sum_{i=1}^ny_i)^2\\
&= SS -\frac{S^2}{n}
\end{aligned}
$$ $$
\begin{aligned}
SSE_{Reduction} &= SSE_{parent}-SSE_{left}-SSE_{right}\\
&= SS_{parent} -\frac{S_{parent}^2}{n_{parent}}-(SS_{left} -\frac{S_{left}^2}{n_{left}})-(SS_{right} -\frac{S_{right}^2}{n_{right}})
\end{aligned}
$$ Since $$n\_{parent} = n\_{left} + n\_{right}\\ SS\_{parent} = SS\_{left} + SS\_{right}$$

Then $$ 
SSE\_{Reduction} = -\frac{S_{parent}^2}{n_{parent}} +\frac{S_{left}^2}{n_{left}} +\frac{S_{right}^2}{n_{right}}\\ max(SSE\_{Reduction}) = max(\frac{S_{left}^2}{n_{left}} +\frac{S_{right}^2}{n_{right}})=min(-\frac{S_{left}^2}{n_{left}} -\frac{S_{right}^2}{n_{right}}) 
$$
